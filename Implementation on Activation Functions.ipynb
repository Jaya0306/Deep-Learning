{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ab3ca8-21a8-4ebe-ac04-61bcb89e788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cffff-2f3f-4c75-9d39-8b4b9df8b475",
   "metadata": {},
   "source": [
    "### Define activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45fd510-3c1b-487f-bc75-bc3e4241ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    \"Leaky ReLU\": nn.LeakyReLU(0.01),\n",
    "    \"PReLU\": nn.PReLU(),\n",
    "    \"RReLU\": nn.RReLU(0.1, 0.3),\n",
    "    \"ELU\": nn.ELU(),\n",
    "    \"Sigmoid\": nn.Sigmoid(),\n",
    "    \"Tanh\": nn.Tanh(),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f64780c-2eb4-4b78-bee0-571788e06e88",
   "metadata": {},
   "source": [
    "### Preprocessing MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc30d2e2-5445-4ff1-843b-31d129dd50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d7fca-bde5-4b4c-9df0-ca6909ced837",
   "metadata": {},
   "source": [
    "### Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1938910-abf6-456f-adb0-d7e26d614973",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59a7d3-98d9-4d10-9bf2-7db56f377b46",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b754cb04-f72b-4db9-a594-0273f532b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation in output (CrossEntropyLoss includes softmax)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c895b-db54-40d5-962e-3bd5f649a177",
   "metadata": {},
   "source": [
    "### Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937ef466-2391-49c2-93dd-5eb888101008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def train_and_evaluate(activation_name, activation_fn):\n",
    "    model = SimpleNN(activation_fn)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    for epoch in range(5):  # Train for 5 epochs\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return training_time, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7a839-6c21-4ed3-bc42-d359a199ac72",
   "metadata": {},
   "source": [
    "### Run Experiments for Different Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26b7b3c-eb27-4add-bd4e-ce9654f1e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with ReLU activation...\n",
      "Training with Leaky ReLU activation...\n",
      "Training with PReLU activation...\n",
      "Training with RReLU activation...\n",
      "Training with ELU activation...\n",
      "Training with Sigmoid activation...\n",
      "Training with Tanh activation...\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, func in activation_functions.items():\n",
    "    print(f\"Training with {name} activation...\")\n",
    "    time_taken, acc = train_and_evaluate(name, func)\n",
    "    results[name] = {\"Time (s)\": round(time_taken, 2), \"Accuracy (%)\": round(acc, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d133a6-d17c-4333-b9ab-3764449c3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Comparison of Activation Functions:\n",
      "\n",
      "ReLU: Time = 123.17s, Accuracy = 97.31%\n",
      "Leaky ReLU: Time = 120.42s, Accuracy = 97.45%\n",
      "PReLU: Time = 122.53s, Accuracy = 97.12%\n",
      "RReLU: Time = 122.66s, Accuracy = 97.08%\n",
      "ELU: Time = 120.66s, Accuracy = 97.39%\n",
      "Sigmoid: Time = 119.47s, Accuracy = 97.08%\n",
      "Tanh: Time = 122.19s, Accuracy = 96.52%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 Comparison of Activation Functions:\\n\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name}: Time = {res['Time (s)']}s, Accuracy = {res['Accuracy (%)']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed88dc-96ca-4410-a73c-4254aea3f1ef",
   "metadata": {},
   "source": [
    "### Insights from Activation Function Performance on MNIST Dataset\n",
    "\n",
    "#### 1️ Performance Insights  \n",
    "- **Leaky ReLU** achieved the **highest accuracy (97.45%)**, slightly outperforming ReLU and ELU.  \n",
    "- **ReLU (97.31%)** and **ELU (97.39%)** performed very similarly, showing that these activation functions work well for MNIST.  \n",
    "- **PReLU and RReLU** had slightly lower accuracy (**~97.1%**), suggesting that **learnable and randomized negative slopes** didn't improve much in this case.  \n",
    "- **Tanh had the lowest accuracy (96.52%)**, likely due to its **vanishing gradient issue**.  \n",
    "\n",
    "#### 2️ Training Time Analysis  \n",
    "- **Sigmoid had the shortest training time (119.47s)**, but it performed worse (**97.08%**) due to **saturation effects**.  \n",
    "- **Leaky ReLU trained slightly faster (120.42s)** while achieving the **highest accuracy**.  \n",
    "- **ReLU had the longest training time (123.17s)**, possibly due to **dead neurons slowing down backpropagation**.  \n",
    "\n",
    "#### 3️ Key Takeaways  \n",
    "- **Leaky ReLU > ReLU > ELU for MNIST** (**highest accuracy + faster training**).  \n",
    "- **Tanh and Sigmoid underperformed**, reinforcing that they are **not ideal for deep networks**.  \n",
    "- **ReLU still remains a strong baseline**, but **Leaky ReLU may be slightly better**.  \n",
    "- **ELU performed well** and might be useful for **more complex datasets** where **zero-centered activations help**.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
